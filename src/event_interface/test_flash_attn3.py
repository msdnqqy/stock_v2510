import torch
import torch.nn.functional as F
from torch.backends.cuda import sdp_kernel, SDPBackend

# ç¡¬ä»¶è®¾å®š
device = "cuda"
dtype = torch.float16
BATCH, SEQ, HEADS, DIM = 4, 4096, 16, 128

q = torch.randn(BATCH, SEQ, HEADS, DIM, device=device, dtype=dtype)
k = torch.randn(BATCH, SEQ, HEADS, DIM, device=device, dtype=dtype)
v = torch.randn(BATCH, SEQ, HEADS, DIM, device=device, dtype=dtype)

print(f"PyTorch Version: {torch.__version__}")
print(f"GPU: {torch.cuda.get_device_name(0)}")

print("\n--- ğŸ•µï¸â€â™‚ï¸ SDPA åç«¯ä¾¦æ¢ ---")

# 1. å¼ºåˆ¶ä½¿ç”¨ Flash Attention
try:
    with sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
        # é¢„çƒ­
        F.scaled_dot_product_attention(q, k, v)

        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(50):
            F.scaled_dot_product_attention(q, k, v)
        end.record()
        torch.cuda.synchronize()
        print(f"âœ… SDPA (å¼ºåˆ¶ FlashAttention): {start.elapsed_time(end):.2f} ms")
except RuntimeError as e:
    print(f"âŒ SDPA æ— æ³•ä½¿ç”¨ FlashAttention: {e}")

# 2. å¼ºåˆ¶ä½¿ç”¨ Math (æ ‡å‡†æ…¢é€Ÿ Attention)
try:
    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(50):  # è·‘å°‘ä¸€ç‚¹ï¼Œå› ä¸ºçœŸçš„å¾ˆæ…¢
            F.scaled_dot_product_attention(q, k, v)
        end.record()
        torch.cuda.synchronize()
        print(f"ğŸ¢ SDPA (å¼ºåˆ¶ Math/æ ‡å‡†): {start.elapsed_time(end):.2f} ms")
except RuntimeError:
    print("æ— æ³•ä½¿ç”¨ Math Attention")