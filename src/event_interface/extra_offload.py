import os
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from json_repair import repair_json
from transformers import AutoConfig
from time import time

# ===== WSL ä¸“å±å†…å­˜ä¼˜åŒ– =====
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["ACCELERATE_DISABLE_RICH"] = "true"
torch.cuda.empty_cache()
torch.backends.cudnn.benchmark = True

# ===== CPU Offload é…ç½® (å…³é”®!) =====
OFFLOAD_DIR = Path("/mnt/d/offload_cache")  # âš ï¸ å¿…é¡»ä½¿ç”¨WSLå¯è®¿é—®çš„ç£ç›˜ç›®å½• (é/tmp!)
OFFLOAD_DIR.mkdir(exist_ok=True, parents=True)


# é…ç½®
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # æ­£å¼æ¨¡å‹ID
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === æ›´æ¿€è¿›ä½†æ›´é«˜æ•ˆçš„ GPU å†…å­˜åˆ†é… ===
if DEVICE == "cuda":
    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    # ä¿å®ˆåªç•™ 0.8~1.0GB ç»™ç³»ç»Ÿï¼ˆWSL é©±åŠ¨å¼€é”€è¾ƒå°ï¼‰
    gpu_mem_gb = max(1, int(total_mem_gb - 0.8))  # 16GB GPU â†’ 15GB
    GPU_MEM = f"{gpu_mem_gb}GB"
else:
    GPU_MEM = "0GB"

# === æ™ºèƒ½å†…å­˜åˆ†é…ç­–ç•¥ ===
MAX_MEMORY = {
    0: GPU_MEM,      # å°½å¯èƒ½ä½¿ç”¨ GPUï¼ˆè‡ªåŠ¨æ£€æµ‹æˆ–ä½ å¯æ‰‹åŠ¨è®¾ä¸º "20GB" ç­‰ï¼‰
    "cpu": "64GB",   # å½“ GPU ä¸è¶³æ—¶ï¼Œç”¨ CPU æ‰¿è½½å‰©ä½™å±‚
    # "disk": "32GB" # ä¸€èˆ¬ä¸å»ºè®®å¯ç”¨ disk offloadï¼ˆææ…¢ï¼‰ï¼Œé™¤éå†…å­˜ä¹Ÿçˆ†äº†
}

print("DEVICE: ",DEVICE)

print(f"ğŸš€ å¯åŠ¨ DeepSeek-R1-0528-Qwen3-8B | Offloadç›®å½•: {OFFLOAD_DIR}")
print(f"ğŸ§  å†…å­˜ç­–ç•¥: GPU={MAX_MEMORY[0]}, CPU={MAX_MEMORY['cpu']}")

# ===== æ™ºèƒ½åŠ è½½æ¨¡å‹ (å¸¦ offload) =====
print(f"ğŸš€ å¯åŠ¨ {MODEL_NAME}")
print(f"ğŸ§  å†…å­˜ç­–ç•¥: GPU={MAX_MEMORY[0]}, CPU={MAX_MEMORY['cpu']}")


from transformers import BitsAndBytesConfig



# === åŠ è½½ Tokenizer ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

print("ğŸ”§ ä¿®å¤æ¨¡å‹é…ç½®...")
config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)
if hasattr(config, 'rope_scaling') and config.rope_scaling:
    if config.rope_scaling.get('rope_type') == 'yarn':
        config.rope_scaling.pop('attn_factor', None)
        config.rope_scaling['factor'] = 4.0
        config.rope_scaling['original_max_position_embeddings'] = 32768

print("ğŸš€ åŠ è½½ 8-bit æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    config=config,  # ä½¿ç”¨ä¿®å¤åçš„é…ç½®
    quantization_config=BitsAndBytesConfig(
        load_in_8bit=True,
        bnb_8bit_compute_dtype=torch.bfloat16,
        bnb_8bit_use_double_quant=True,
    ),
    device_map="cuda:0",  # å¼ºåˆ¶å…¨ GPU
    trust_remote_code=True,
    # use_cache=True,
    attn_implementation="flash_attention_2",  # æ˜¾å¼å¯ç”¨
)

# åŠ é€Ÿç­–ç•¥
model = torch.compile(model, mode="reduce-overhead")

# åˆ›å»º pipeline (æ—  device å‚æ•°)
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    # device=0,  # æ˜¾å¼æŒ‡å®š GPU
    framework="pt",
    batch_size=1,
    # ç”Ÿæˆå‚æ•°
    max_new_tokens=128,
    temperature=0.2,
    top_p=0.85,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)



def extract_causal_relations(text: str, max_retries=2):
    """å¸¦ CPU offload ä¼˜åŒ–çš„å› æœæ¨æ–­"""

    FEW_SHOT_EXAMPLES = [
        # ç¤ºä¾‹1: ç»æµæ”¿ç­–å½±å“ (å¤šå®ä½“)
        {
            "input": "å¤®è¡Œå®£å¸ƒåŠ æ¯0.5%ï¼Œå¯¼è‡´æŠµæŠ¼è´·æ¬¾åˆ©ç‡ä¸Šå‡ï¼ŒåŒæ—¶å‚¨è“„è´¦æˆ·æ”¶ç›Šå¢åŠ ã€‚",
            "output": [
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "æŠµæŠ¼è´·æ¬¾åˆ©ç‡",
                    "effect": "è´Ÿé¢ï¼šä¸Šå‡"
                },
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "å‚¨è“„è´¦æˆ·æ”¶ç›Š",
                    "effect": "æ­£é¢ï¼šå¢åŠ "
                }
            ]
        },

        # ç¤ºä¾‹2: åŒ»ç–—å¹²é¢„æ•ˆæœ (éšæ€§å› æœ)
        {
            "input": "ä¸´åºŠè¯•éªŒæ˜¾ç¤ºï¼Œæ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆåï¼Œå‚ä¸è€…éª¨æŠ˜é£é™©é™ä½äº†22%ã€‚",
            "output": [
                {
                    "event": "æ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆ",
                    "entity": "éª¨æŠ˜é£é™©",
                    "effect": "æ­£é¢ï¼šé™ä½22%"
                }
            ]
        },

        # ç¤ºä¾‹3: æ— æ˜ç¡®å› æœå…³ç³» (è¾¹ç•Œæƒ…å†µ)
        {
            "input": "ä¼šè®®å°†äºä¸‹å‘¨ä¸‰ä¸¾è¡Œï¼Œåœ°ç‚¹åœ¨æ€»éƒ¨å¤§æ¥¼3æ¥¼ä¼šè®®å®¤ã€‚",
            "output": []
        },

        # ç¤ºä¾‹4: å¤æ‚é“¾å¼å› æœ (é«˜çº§æ¨¡å¼)
        {
            "input": "ä¾›åº”é“¾ä¸­æ–­å¼•å‘èŠ¯ç‰‡çŸ­ç¼ºï¼Œè¿«ä½¿æ±½è½¦åˆ¶é€ å•†å‡äº§ï¼Œè¿›è€Œå¯¼è‡´äºŒæ‰‹è½¦ä»·æ ¼ä¸Šæ¶¨30%ã€‚",
            "output": [
                {
                    "event": "ä¾›åº”é“¾ä¸­æ–­",
                    "entity": "èŠ¯ç‰‡ä¾›åº”",
                    "effect": "è´Ÿé¢ï¼šçŸ­ç¼º"
                },
                {
                    "event": "èŠ¯ç‰‡çŸ­ç¼º",
                    "entity": "æ±½è½¦äº§é‡",
                    "effect": "è´Ÿé¢ï¼šå‡äº§"
                },
                {
                    "event": "æ±½è½¦å‡äº§",
                    "entity": "äºŒæ‰‹è½¦ä»·æ ¼",
                    "effect": "è´Ÿé¢ï¼šä¸Šæ¶¨30%"
                }
            ]
        }
    ]

    # æ„å»º prompt (åŒå‰æ–‡)
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªå› æœæ¨æ–­ä¸“å®¶ç³»ç»Ÿ...\n### å‚è€ƒç¤ºä¾‹:\n"
    for ex in FEW_SHOT_EXAMPLES:
        system_prompt += f"è¾“å…¥: \"{ex['input']}\"\nè¾“å‡º: {json.dumps(ex['output'])}\n"

    user_prompt = f"### å¾…åˆ†ææ–‡æœ¬\n\"\"\"{text}\"\"\"\n### ä¸¥æ ¼JSONè¾“å‡º:"

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # prompt = tokenizer.apply_chat_template(
    #     messages,
    #     tokenize=False,
    #     add_generation_prompt=True
    # )

    # Offload ä¸“ç”¨é‡è¯•æœºåˆ¶
    for attempt in range(max_retries + 1):
        try:
            # å…³é”®: æ¸…ç†ç¼“å­˜é¿å… offload å†…å­˜æ³„æ¼
            torch.cuda.empty_cache()
            if attempt > 0:
                print(f"  â™»ï¸  é‡è¯• #{attempt} (æ¸…ç†ç¼“å­˜å)")

            start_time = time()
            response = text_generator(
                messages,  # â† ç›´æ¥ä¼ å…¥æ¶ˆæ¯
                return_full_text=False
            )[0]['generated_text']
            duration = time() - start_time

            # é€Ÿåº¦ç›‘æ§
            gen_tokens = len(tokenizer.encode(response))
            print(f"âš¡ é€Ÿåº¦: {gen_tokens / duration:.1f} token/s ({gen_tokens} tokens in {duration:.2f}s)")

            # ä¿®å¤ JSON
            repaired = repair_json(response, return_objects=True)
            return _validate_results(repaired)

        except Exception as e:
            print(f"  âŒ  å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
            torch.cuda.empty_cache()

    print("  ğŸ›‘  æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
    return []


def _validate_results(results):
    """éªŒè¯ç»“æœç»“æ„ (offload æ—¶æ›´ä¸¥æ ¼)"""
    if not isinstance(results, list):
        return []

    valid_items = []
    for item in results:
        if not (isinstance(item, dict) and
                all(k in item for k in ['event', 'entity', 'effect'])):
            continue

        # ç¡®ä¿ effect åŒ…å«ææ€§
        if "æ­£é¢" not in item['effect'] and "è´Ÿé¢" not in item['effect']:
            item['effect'] = f"ä¸­æ€§ï¼š{item['effect']}"

        valid_items.append(item)
    return valid_items


# ===== æ™ºèƒ½ç›‘æ§å·¥å…· =====
def monitor_resources():
    """å®æ—¶ç›‘æ§ WSL èµ„æºä½¿ç”¨"""
    print("\n" + "=" * 50)
    print("ğŸ“Š èµ„æºç›‘æ§æŠ¥å‘Š")
    print("-" * 50)

    # GPU æ˜¾å­˜
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"GPU æ˜¾å­˜: å·²åˆ†é… {allocated:.1f}GB / ä¿ç•™ {reserved:.1f}GB")

    # CPU å†…å­˜
    import psutil
    mem = psutil.virtual_memory()
    print(f"CPU å†…å­˜: {mem.used / 1e9:.1f}GB / {mem.total / 1e9:.1f}GB ({mem.percent}%)")

    # Offload ç›®å½•å¤§å°
    offload_size = sum(f.stat().st_size for f in OFFLOAD_DIR.glob('**/*') if f.is_file()) / 1e9
    print(f"Offload ç©ºé—´: {offload_size:.1f}GB (ç›®å½•: {OFFLOAD_DIR})")

    # ç£ç›˜ç©ºé—´
    disk = psutil.disk_usage(str(OFFLOAD_DIR))
    print(f"ç£ç›˜ç©ºé—´: {disk.used / 1e9:.1f}GB / {disk.total / 1e9:.1f}GB ({disk.percent}%)")
    print("=" * 50)


# ===== æµ‹è¯•è¿è¡Œ =====
if __name__ == "__main__":
    monitor_resources()

    sample_text = (
        "å…¨çƒæ°”å€™å˜æš–å¯¼è‡´åŒ—æå†°ç›–èåŒ–åŠ é€Ÿï¼Œå¼•å‘æµ·å¹³é¢ä¸Šå‡å¨èƒæ²¿æµ·åŸå¸‚ã€‚"
        "åŒæ—¶ï¼Œå¯å†ç”Ÿèƒ½æºæŠ•èµ„å¢é•¿é™ä½äº†å¤ªé˜³èƒ½æ¿æˆæœ¬ï¼Œæ¨åŠ¨äº†ç»¿è‰²æŠ€æœ¯æ™®åŠã€‚"
    )

    print("\nğŸ” åˆ†ææ–‡æœ¬:", sample_text)
    results = extract_causal_relations(sample_text)

    print("\nâœ… æå–ç»“æœ:")
    for i, rel in enumerate(results, 1):
        print(f"\n{i}. äº‹ä»¶: {rel['event']}")
        print(f"   å®ä½“: {rel['entity']}")
        print(f"   å½±å“: {rel['effect']}")

    # ä¿å­˜ç»“æœ
    with open("deepseek_causal_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ç»“æœä¿å­˜è‡³: deepseek_causal_results.json")

    monitor_resources()