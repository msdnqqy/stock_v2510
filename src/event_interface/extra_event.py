import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from json_repair import repair_json

# é…ç½®ï¼ˆä½¿ç”¨Qwen1.5-1.8Bä½œä¸ºæ›¿ä»£ï¼ŒDeepSeekç‰ˆæœ¬å‘å¸ƒåæ›¿æ¢MODEL_NAMEï¼‰
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # æ›¿æ¢ä¸º"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"å½“å¯ç”¨æ—¶
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("DEVICE:\t",DEVICE)

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    dtype=torch.bfloat16 if DEVICE == "cuda" else torch.float32,
    # device_map="auto",
    trust_remote_code=True,
    use_cache=True
)

text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=384,  # å¢åŠ ä»¥é€‚åº” few-shot
    temperature=0.2,  # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)


def extract_causal_relations(text: str, max_retries=2):
    """
    å¢å¼ºç‰ˆå› æœå…³ç³»æå– (å¸¦ few-shot å­¦ä¹ )

    :param text: è¾“å…¥æ–‡æœ¬
    :param max_retries: JSON è§£æå¤±è´¥é‡è¯•æ¬¡æ•°
    :return: ç»“æ„åŒ–å› æœå…³ç³»åˆ—è¡¨
    """

    # ===== ä¸“ä¸šçº§ Few-Shot ç¤ºä¾‹åº“ =====
    FEW_SHOT_EXAMPLES = [
        # ç¤ºä¾‹1: ç»æµæ”¿ç­–å½±å“ (å¤šå®ä½“)
        {
            "input": "å¤®è¡Œå®£å¸ƒåŠ æ¯0.5%ï¼Œå¯¼è‡´æŠµæŠ¼è´·æ¬¾åˆ©ç‡ä¸Šå‡ï¼ŒåŒæ—¶å‚¨è“„è´¦æˆ·æ”¶ç›Šå¢åŠ ã€‚",
            "output": [
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "æŠµæŠ¼è´·æ¬¾åˆ©ç‡",
                    "effect": "è´Ÿé¢ï¼šä¸Šå‡"
                },
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "å‚¨è“„è´¦æˆ·æ”¶ç›Š",
                    "effect": "æ­£é¢ï¼šå¢åŠ "
                }
            ]
        },

        # ç¤ºä¾‹2: åŒ»ç–—å¹²é¢„æ•ˆæœ (éšæ€§å› æœ)
        {
            "input": "ä¸´åºŠè¯•éªŒæ˜¾ç¤ºï¼Œæ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆåï¼Œå‚ä¸è€…éª¨æŠ˜é£é™©é™ä½äº†22%ã€‚",
            "output": [
                {
                    "event": "æ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆ",
                    "entity": "éª¨æŠ˜é£é™©",
                    "effect": "æ­£é¢ï¼šé™ä½22%"
                }
            ]
        },

        # ç¤ºä¾‹3: æ— æ˜ç¡®å› æœå…³ç³» (è¾¹ç•Œæƒ…å†µ)
        {
            "input": "ä¼šè®®å°†äºä¸‹å‘¨ä¸‰ä¸¾è¡Œï¼Œåœ°ç‚¹åœ¨æ€»éƒ¨å¤§æ¥¼3æ¥¼ä¼šè®®å®¤ã€‚",
            "output": []
        },

        # ç¤ºä¾‹4: å¤æ‚é“¾å¼å› æœ (é«˜çº§æ¨¡å¼)
        {
            "input": "ä¾›åº”é“¾ä¸­æ–­å¼•å‘èŠ¯ç‰‡çŸ­ç¼ºï¼Œè¿«ä½¿æ±½è½¦åˆ¶é€ å•†å‡äº§ï¼Œè¿›è€Œå¯¼è‡´äºŒæ‰‹è½¦ä»·æ ¼ä¸Šæ¶¨30%ã€‚",
            "output": [
                {
                    "event": "ä¾›åº”é“¾ä¸­æ–­",
                    "entity": "èŠ¯ç‰‡ä¾›åº”",
                    "effect": "è´Ÿé¢ï¼šçŸ­ç¼º"
                },
                {
                    "event": "èŠ¯ç‰‡çŸ­ç¼º",
                    "entity": "æ±½è½¦äº§é‡",
                    "effect": "è´Ÿé¢ï¼šå‡äº§"
                },
                {
                    "event": "æ±½è½¦å‡äº§",
                    "entity": "äºŒæ‰‹è½¦ä»·æ ¼",
                    "effect": "è´Ÿé¢ï¼šä¸Šæ¶¨30%"
                }
            ]
        }
    ]

    # ===== æ„å»º Few-Shot Prompt =====
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªå› æœæ¨æ–­ä¸“å®¶ç³»ç»Ÿï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. ä»…æå–æ–‡æœ¬ä¸­æ˜ç¡®çš„å› æœå…³ç³»ï¼ˆäº‹ä»¶â†’å®ä½“â†’å½±å“ï¼‰
2. å½±å“æè¿°å¿…é¡»åŒ…å«ï¼š
   - ææ€§ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
   - å…·ä½“å˜åŒ–ï¼ˆå¦‚"å¢åŠ 15%"ã€"æ˜¾è‘—é™ä½"ï¼‰
3. è¾“å‡ºä¸¥æ ¼çš„JSONæ•°ç»„æ ¼å¼ï¼Œæ— é¢å¤–æ–‡æœ¬
4. æ— å› æœå…³ç³»æ—¶è¿”å›ç©ºæ•°ç»„ []

### å‚è€ƒç¤ºä¾‹ï¼ˆå­¦ä¹ æ ¼å¼å’Œé€»è¾‘ï¼‰:
"""

    # æ·»åŠ  few-shot ç¤ºä¾‹
    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):
        system_prompt += f"\nç¤ºä¾‹ #{i}:\n"
        system_prompt += f"è¾“å…¥: \"{ex['input']}\"\n"
        system_prompt += f"è¾“å‡º: {json.dumps(ex['output'], ensure_ascii=False)}\n"

    # ç”¨æˆ·æŸ¥è¯¢
    user_prompt = f"""### å¾…åˆ†ææ–‡æœ¬
\"\"\"{text}\"\"\"\n
### ä¸¥æ ¼JSONè¾“å‡ºï¼ˆä»…JSONï¼Œæ— å…¶ä»–å†…å®¹ï¼‰:"""

    # æ„å»ºå¯¹è¯
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries + 1):
        try:
            # ç”Ÿæˆå“åº”
            response = text_generator(
                prompt,
                return_full_text=False,
                clean_up_tokenization_spaces=True
            )[0]['generated_text']

            # ä¿®å¤ JSON
            repaired = repair_json(response, return_objects=True)

            # éªŒè¯ç»“æ„
            if isinstance(repaired, list):
                # éªŒè¯æ¯ä¸ªæ¡ç›®
                valid_items = []
                for item in repaired:
                    if isinstance(item, dict) and {'event', 'entity', 'effect'}.issubset(item.keys()):
                        valid_items.append(item)
                return valid_items

            if attempt < max_retries:
                print(f"  âš ï¸ æ ¼å¼æ— æ•ˆ (å°è¯• {attempt + 1}/{max_retries + 1})ï¼Œé‡è¯•ä¸­...")
                continue
            return []

        except Exception as e:
            print(f"  âŒ å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
            if attempt == max_retries:
                torch.cuda.empty_cache()
                return []

    return []


# ===== é«˜çº§æµ‹è¯•å¥—ä»¶ =====
TEST_CASES = [
    {
        "text": "æ–°ç¯ä¿æ³•è§„å®æ–½åï¼Œå·¥å‚æ’æ”¾é‡å‡å°‘äº†40%ï¼Œä½†åˆè§„æˆæœ¬å¢åŠ äº†ä¸­å°ä¼ä¸šè´Ÿæ‹…ã€‚",
        "expected": 2  # é¢„æœŸ2ä¸ªå› æœå…³ç³»
    },
    {
        "text": "ç ”ç©¶å‘ç°ï¼Œæ¯å¤©é˜…è¯»30åˆ†é’Ÿå¯ä½¿è®¤çŸ¥èƒ½åŠ›æå‡15%ï¼Œè€Œä¹…åä¸åŠ¨åˆ™é™ä½å¿ƒè¡€ç®¡å¥åº·ã€‚",
        "expected": 2
    },
    {
        "text": "ä¼šè®®è®°å½•å­˜æ¡£äºå…±äº«æ–‡ä»¶å¤¹ï¼Œé¡¹ç›®æˆªæ­¢æ—¥æœŸæ˜¯ä¸‹å‘¨äº”ã€‚",
        "expected": 0  # æ— å› æœå…³ç³»
    },
    {
        "text": "5Gç½‘ç»œéƒ¨ç½²åŠ é€Ÿäº†ç‰©è”ç½‘è®¾å¤‡æ™®åŠï¼Œå¼•å‘æ•°æ®å®‰å…¨æ‹…å¿§ï¼Œä¿ƒä½¿æ–°åŠ å¯†æ ‡å‡†å‡ºå°ã€‚",
        "expected": 3  # é“¾å¼å› æœ
    }
]

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”¬ å› æœæ¨æ–­å¼•æ“ (Few-Shot å¢å¼ºç‰ˆ)")
    print(f"ğŸš€ è®¾å¤‡: {DEVICE.upper()} | æ¨¡å‹: {MODEL_NAME}")
    print("=" * 60)

    for i, case in enumerate(TEST_CASES, 1):
        print(f"\nğŸ§ª æµ‹è¯• #{i}:")
        print(f"ğŸ“ æ–‡æœ¬: \"{case['text']}\"")

        results = extract_causal_relations(case['text'])

        print(f"\nâœ… å‘ç° {len(results)} ä¸ªå› æœå…³ç³» (é¢„æœŸ: {case['expected']}):")
        for j, rel in enumerate(results, 1):
            print(f"  {j}. äº‹ä»¶: {rel['event']}")
            print(f"     å®ä½“: {rel['entity']}")
            print(f"     å½±å“: {rel['effect']}")

        # è¯„ä¼°å‡†ç¡®æ€§
        accuracy = min(len(results), case['expected']) / max(len(results), case['expected'], 1)
        status = "ğŸŸ¢ é€šè¿‡" if abs(len(results) - case['expected']) <= 1 else "ğŸŸ¡ è­¦å‘Š" if accuracy > 0.7 else "ğŸ”´ å¤±è´¥"
        print(f"\nğŸ“Š è¯„ä¼°: {status} (å‡†ç¡®ç‡: {accuracy:.0%})")

    # ä¿å­˜å®Œæ•´ç»“æœ
    full_results = []
    for case in TEST_CASES:
        full_results.append({
            "input": case['text'],
            "output": extract_causal_relations(case['text'])
        })

    with open("causal_analysis_results.json", "w", encoding="utf-8") as f:
        json.dump(full_results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: causal_analysis_results.json")