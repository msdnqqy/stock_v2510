#!/usr/bin/env python3
"""
DeepSeek-R1-0528-Qwen3-8B å› æœæ¨æ–­ä¼˜åŒ–ç‰ˆ
âœ… ä¿®å¤ YARN RoPE å…¼å®¹æ€§é—®é¢˜
âœ… å¯ç”¨ Flash Attention 2
âœ… 8-bit é‡åŒ– + å…¨ GPU æ¨ç†
âœ… WSL2 ä¸“å±æ€§èƒ½ä¼˜åŒ–
âœ… å®æ—¶é€Ÿåº¦ç›‘æ§ (token/s)
âœ… å†…å­˜æ³„æ¼é˜²æŠ¤

ç¯å¢ƒè¦æ±‚:
- Python 3.12+
- torch 2.3.0+cu121 (å®˜æ–¹ç‰ˆæœ¬)
- transformers>=4.45.0
- flash-attn>=2.5.0
- bitsandbytes
- json-repair
- psutil
"""

import os
import json
import torch
import time
from pathlib import Path
from typing import List, Dict, Any, Union
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoConfig,
    BitsAndBytesConfig
)
from json_repair import repair_json
import psutil

# ===== WSL2 ä¸“å±æ€§èƒ½ä¼˜åŒ– =====
os.environ.update({
    "CUDA_LAUNCH_BLOCKING": "0",  # ç¦ç”¨åŒæ­¥è°ƒè¯•
    "PYTORCH_CUDA_ALLOC_CONF": "backend:cudaMallocAsync",  # å¼‚æ­¥å†…å­˜åˆ†é…
    "TF_ENABLE_ONEDNN_OPTS": "0",  # ç¦ç”¨ TensorFlow å†²çª
    "TOKENIZERS_PARALLELISM": "false",  # é¿å… tokenizer å†²çª
    "ACCELERATE_DISABLE_RICH": "true",  # ç¦ç”¨ rich è¾“å‡º
    "BITSANDBYTES_NOWELCOME": "1",  # ç¦ç”¨ bitsandbytes æ¬¢è¿æ¶ˆæ¯
    "PYTORCH_NO_CUDA_MEMORY_CACHING": "0"  # å¯ç”¨ç¼“å­˜
})

# PyTorch æ€§èƒ½ä¼˜åŒ–
torch.backends.cuda.enable_flash_sdp(True)  # å¯ç”¨ Flash Attention
torch.backends.cuda.enable_mem_efficient_sdp(False)  # ç¦ç”¨ä½æ•ˆæ¨¡å¼
torch.backends.cuda.enable_math_sdp(False)  # ç¦ç”¨æ•°å­¦æ¨¡å¼
torch.backends.cudnn.benchmark = True  # å¯ç”¨ cuDNN benchmark
torch.set_float32_matmul_precision('high')  # å¯ç”¨ TF32 åŠ é€Ÿ
torch.cuda.empty_cache()  # å¯åŠ¨æ—¶æ¸…ç†ç¼“å­˜

# ===== é…ç½®å‚æ•° =====
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
OFFLOAD_DIR = Path("/mnt/d/offload_cache")  # WSL2 å¯è®¿é—®çš„ç£ç›˜ç›®å½•
OFFLOAD_DIR.mkdir(exist_ok=True, parents=True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸš€ ç³»ç»Ÿåˆå§‹åŒ– | è®¾å¤‡: {DEVICE} | Offloadç›®å½•: {OFFLOAD_DIR}")
print(f"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__} | CUDA ç‰ˆæœ¬: {torch.version.cuda}")


# ===== ä¸‰é‡ä¿®å¤ï¼šå®‰å…¨åŠ è½½æ¨¡å‹ =====
def load_model_safely(model_name: str) -> AutoModelForCausalLM:
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œè§£å†³ YARN RoPE å…¼å®¹æ€§é—®é¢˜"""
    print("\n" + "=" * 60)
    print("ğŸ”§ ä¸‰é‡ä¿®å¤ï¼šå®‰å…¨åŠ è½½æ¨¡å‹")
    print("-" * 60)

    # ç¬¬ä¸€é‡ä¿®å¤ï¼šåˆ›å»ºå…¼å®¹çš„é…ç½®
    print("âœ… ç¬¬ä¸€é‡ä¿®å¤: åˆ›å»ºå…¼å®¹é…ç½®...")
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)

    # æ·±åº¦ä¿®å¤ YARN RoPE é…ç½®
    if hasattr(config, 'rope_scaling') and config.rope_scaling:
        print("  ğŸ› ï¸  æ·±åº¦ä¿®å¤ YARN RoPE é…ç½®:")
        original_keys = set(config.rope_scaling.keys())

        # åˆ›å»ºæ ‡å‡†å…¼å®¹é…ç½®
        config.rope_scaling = {
            "rope_type": "yarn",
            "factor": 4.0,  # 4å€ä¸Šä¸‹æ–‡æ‰©å±•
            "original_max_position_embeddings": 32768,  # åŸå§‹æœ€å¤§é•¿åº¦
            "beta_fast": 32,
            "beta_slow": 1
        }

        removed_keys = original_keys - set(config.rope_scaling.keys())
        print(f"  ğŸ”‘  ç§»é™¤ä¸å…¼å®¹å­—æ®µ: {removed_keys}")
        print(f"  âœ…  æ–°é…ç½®: {config.rope_scaling}")

    # ç¬¬äºŒé‡ä¿®å¤ï¼š8-bit é‡åŒ–é…ç½®
    print("\nâœ… ç¬¬äºŒé‡ä¿®å¤: é…ç½® 8-bit é‡åŒ–...")
    quant_config = BitsAndBytesConfig(
        load_in_8bit=True,
        bnb_8bit_compute_dtype=torch.bfloat16,
        bnb_8bit_use_double_quant=True,
        bnb_8bit_quant_type="nf4"  # æ›´é«˜æ•ˆçš„é‡åŒ–ç±»å‹
    )

    # ç¬¬ä¸‰é‡ä¿®å¤ï¼šåŠ è½½æ¨¡å‹
    print("\nâœ… ç¬¬ä¸‰é‡ä¿®å¤: åŠ è½½æ¨¡å‹ (å¯ç”¨ Flash Attention 2)...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            config=config,
            quantization_config=quant_config,
            device_map="cuda:0",
            trust_remote_code=True,
            attn_implementation="flash_attention_2",  # å¼ºåˆ¶å¯ç”¨
            dtype=torch.bfloat16,
            use_safetensors=True,
            ignore_mismatched_sizes=True,  # å¿½ç•¥é…ç½®ä¸åŒ¹é…
            low_cpu_mem_usage=True
        )
        print("ğŸ‰ ä¸‰é‡ä¿®å¤æˆåŠŸï¼æ¨¡å‹åŠ è½½å®Œæˆ")
        return model

    except Exception as e:
        print(f"âŒ ä¸‰é‡ä¿®å¤å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
        print("ğŸ”„ å›é€€åˆ°åŸºç¡€é…ç½®...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quant_config,
            device_map="cuda:0",
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
            ignore_mismatched_sizes=True
        )
        print("ğŸŸ¡ å¤‡ç”¨æ–¹æ¡ˆåŠ è½½æˆåŠŸ")
        return model


# ===== é«˜æ€§èƒ½ç”Ÿæˆå‡½æ•° =====
def generate_response(
        model: AutoModelForCausalLM,
        tokenizer: AutoTokenizer,
        messages: List[Dict[str, str]],
        max_new_tokens: int = 128
) -> tuple[str, int, float]:
    """
    é«˜æ€§èƒ½æ–‡æœ¬ç”Ÿæˆï¼ˆç»•è¿‡ pipeline ç“¶é¢ˆï¼‰

    Args:
        model: åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°

    Returns:
        (response_text, generated_tokens, duration_seconds)
    """
    # 1. æ„å»º prompt
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # 2. Tokenize å¹¶ç§»è‡³ GPU
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    ).to("cuda")

    # 3. ç”Ÿæˆæ–‡æœ¬ï¼ˆå¯ç”¨ Flash Attentionï¼‰
    torch.cuda.synchronize()  # ç¡®ä¿ GPU æ“ä½œåŒæ­¥
    start_time = time.time()

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦
        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.2,
                top_p=0.85,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True,  # å¯ç”¨ KV ç¼“å­˜
                return_dict_in_generate=True,
                output_scores=False
            )

    torch.cuda.synchronize()  # ç¡®ä¿ç”Ÿæˆå®Œæˆ
    duration = time.time() - start_time

    # 4. è§£ç å“åº”ï¼ˆè·³è¿‡è¾“å…¥éƒ¨åˆ†ï¼‰
    input_length = inputs.input_ids.shape[1]
    generated_tokens = outputs.sequences.shape[1] - input_length
    response = tokenizer.decode(
        outputs.sequences[0, input_length:],
        skip_special_tokens=True
    )

    return response, generated_tokens, duration


# ===== å› æœæ¨æ–­å‡½æ•° =====
def extract_causal_relations(
        model: AutoModelForCausalLM,
        tokenizer: AutoTokenizer,
        text: str,
        max_retries: int = 1
) -> List[Dict[str, str]]:
    """
    ä»æ–‡æœ¬ä¸­æå–å› æœå…³ç³»

    Args:
        model: åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        text: å¾…åˆ†ææ–‡æœ¬
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        å› æœå…³ç³»åˆ—è¡¨
    """
    # å°‘æ ·æœ¬ç¤ºä¾‹
    FEW_SHOT_EXAMPLES = [
        {
            "input": "å¤®è¡Œå®£å¸ƒåŠ æ¯0.5%ï¼Œå¯¼è‡´æŠµæŠ¼è´·æ¬¾åˆ©ç‡ä¸Šå‡ï¼ŒåŒæ—¶å‚¨è“„è´¦æˆ·æ”¶ç›Šå¢åŠ ã€‚",
            "output": [
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "æŠµæŠ¼è´·æ¬¾åˆ©ç‡",
                    "effect": "è´Ÿé¢ï¼šä¸Šå‡"
                },
                {
                    "event": "å¤®è¡ŒåŠ æ¯0.5%",
                    "entity": "å‚¨è“„è´¦æˆ·æ”¶ç›Š",
                    "effect": "æ­£é¢ï¼šå¢åŠ "
                }
            ]
        },
        {
            "input": "ä¸´åºŠè¯•éªŒæ˜¾ç¤ºï¼Œæ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆåï¼Œå‚ä¸è€…éª¨æŠ˜é£é™©é™ä½äº†22%ã€‚",
            "output": [
                {
                    "event": "æ¯æ—¥æœç”¨ç»´ç”Ÿç´ Dè¡¥å……å‰‚6ä¸ªæœˆ",
                    "entity": "éª¨æŠ˜é£é™©",
                    "effect": "æ­£é¢ï¼šé™ä½22%"
                }
            ]
        },
        {
            "input": "ä¼šè®®å°†äºä¸‹å‘¨ä¸‰ä¸¾è¡Œï¼Œåœ°ç‚¹åœ¨æ€»éƒ¨å¤§æ¥¼3æ¥¼ä¼šè®®å®¤ã€‚",
            "output": []
        },
        {
            "input": "ä¾›åº”é“¾ä¸­æ–­å¼•å‘èŠ¯ç‰‡çŸ­ç¼ºï¼Œè¿«ä½¿æ±½è½¦åˆ¶é€ å•†å‡äº§ï¼Œè¿›è€Œå¯¼è‡´äºŒæ‰‹è½¦ä»·æ ¼ä¸Šæ¶¨30%ã€‚",
            "output": [
                {
                    "event": "ä¾›åº”é“¾ä¸­æ–­",
                    "entity": "èŠ¯ç‰‡ä¾›åº”",
                    "effect": "è´Ÿé¢ï¼šçŸ­ç¼º"
                },
                {
                    "event": "èŠ¯ç‰‡çŸ­ç¼º",
                    "entity": "æ±½è½¦äº§é‡",
                    "effect": "è´Ÿé¢ï¼šå‡äº§"
                },
                {
                    "event": "æ±½è½¦å‡äº§",
                    "entity": "äºŒæ‰‹è½¦ä»·æ ¼",
                    "effect": "è´Ÿé¢ï¼šä¸Šæ¶¨30%"
                }
            ]
        }
    ]

    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå› æœæ¨æ–­ä¸“å®¶ç³»ç»Ÿã€‚è¯·ä»æ–‡æœ¬ä¸­ç²¾ç¡®è¯†åˆ«å› æœå…³ç³»ï¼Œéµå¾ªä¸¥æ ¼è§„åˆ™ï¼š\n"
        "1. ä»…å½“å­˜åœ¨æ˜ç¡®å› æœåŠ¨è¯ï¼ˆå¯¼è‡´ã€å¼•å‘ã€é™ä½ã€æ¨åŠ¨ç­‰ï¼‰æ—¶æ‰æå–\n"
        "2. æ¯ä¸ªå› æœå…³ç³»åŒ…å«ä¸‰ä¸ªå­—æ®µï¼ševentï¼ˆåŸå› äº‹ä»¶ï¼‰ã€entityï¼ˆå—å½±å“å®ä½“ï¼‰ã€effectï¼ˆå½±å“æè¿°ï¼‰\n"
        "3. effect å¿…é¡»åŒ…å«ææ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰å’Œå…·ä½“å˜åŒ–\n"
        "4. æ— æ˜ç¡®å› æœå…³ç³»æ—¶è¿”å›ç©ºåˆ—è¡¨ []\n"
        "5. ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼Œæ— ä»»ä½•é¢å¤–æ–‡æœ¬\n\n"
        "### å‚è€ƒç¤ºä¾‹:\n"
    )

    for ex in FEW_SHOT_EXAMPLES:
        system_prompt += f"è¾“å…¥: \"{ex['input']}\"\nè¾“å‡º: {json.dumps(ex['output'], ensure_ascii=False)}\n\n"

    # æ„å»ºç”¨æˆ·æç¤º
    user_prompt = f"### å¾…åˆ†ææ–‡æœ¬\n\"\"\"{text}\"\"\"\n### ä¸¥æ ¼JSONè¾“å‡º:"

    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # ç”Ÿæˆå“åº”
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                print(f"  â™»ï¸  é‡è¯• #{attempt} (æ¸…ç†ç¼“å­˜å)")
                torch.cuda.empty_cache()

            response, generated_tokens, duration = generate_response(
                model,
                tokenizer,
                messages,
                max_new_tokens=128
            )

            # è®¡ç®—å¹¶æ˜¾ç¤ºé€Ÿåº¦
            tokens_per_sec = generated_tokens / duration if duration > 0 else 0
            print(f"ğŸš€ é€Ÿåº¦: {tokens_per_sec:.1f} token/s ({generated_tokens} tokens in {duration:.2f}s)")
            print(f"ğŸ“ æ¨¡å‹å“åº”: {response[:150]}..." if len(response) > 150 else f"ğŸ“ æ¨¡å‹å“åº”: {response}")

            # ä¿®å¤ JSON
            repaired = repair_json(response, return_objects=True)
            return _validate_results(repaired)

        except Exception as e:
            print(f"  âŒ  å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
            torch.cuda.empty_cache()

    print("  ğŸ›‘  æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
    return []


def _validate_results(results: Any) -> List[Dict[str, str]]:
    """éªŒè¯å¹¶æ¸…ç†ç»“æœ"""
    if not isinstance(results, list):
        return []

    valid_items = []
    for item in results:
        if not (isinstance(item, dict) and
                all(k in item for k in ['event', 'entity', 'effect'])):
            continue

        # ç¡®ä¿ effect åŒ…å«ææ€§
        effect = item['effect']
        if "æ­£é¢" not in effect and "è´Ÿé¢" not in effect:
            # å°è¯•æ¨æ–­ææ€§
            negative_words = ["ä¸Šå‡", "å¢åŠ ", "ä¸Šæ¶¨", "æ¶åŒ–", "é™ä½", "å‡å°‘", "ä¸‹é™", "æ¶ˆå¤±"]
            if any(word in effect for word in negative_words):
                effect = f"è´Ÿé¢ï¼š{effect}"
            else:
                effect = f"æ­£é¢ï¼š{effect}"

        valid_items.append({
            "event": item['event'],
            "entity": item['entity'],
            "effect": effect
        })

    return valid_items


# ===== èµ„æºç›‘æ§ =====
def monitor_resources() -> Dict[str, Any]:
    """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    report = {}

    # GPU æ˜¾å­˜
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        report['gpu'] = {
            'allocated_gb': allocated,
            'reserved_gb': reserved,
            # 'utilization': torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0
        }
        print(f"ğŸ“Š GPU æ˜¾å­˜: å·²åˆ†é… {allocated:.1f}GB / ä¿ç•™ {reserved:.1f}GB |")

    # CPU å†…å­˜
    mem = psutil.virtual_memory()
    report['cpu'] = {
        'used_gb': mem.used / 1e9,
        'total_gb': mem.total / 1e9,
        'percent': mem.percent
    }
    print(f"ğŸ§  CPU å†…å­˜: {mem.used / 1e9:.1f}GB / {mem.total / 1e9:.1f}GB ({mem.percent}%)")

    # Offload ç›®å½•å¤§å°
    offload_size = sum(f.stat().st_size for f in OFFLOAD_DIR.glob('**/*') if f.is_file()) / 1e9
    report['offload'] = {
        'size_gb': offload_size,
        'path': str(OFFLOAD_DIR)
    }
    print(f"ğŸ’¾ Offload ç©ºé—´: {offload_size:.1f}GB (ç›®å½•: {OFFLOAD_DIR})")

    # ç£ç›˜ç©ºé—´
    disk = psutil.disk_usage(str(OFFLOAD_DIR))
    report['disk'] = {
        'used_gb': disk.used / 1e9,
        'total_gb': disk.total / 1e9,
        'percent': disk.percent
    }
    print(f"ğŸ’½ ç£ç›˜ç©ºé—´: {disk.used / 1e9:.1f}GB / {disk.total / 1e9:.1f}GB ({disk.percent}%)")

    return report


# ===== ä¸»ç¨‹åº =====
def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸš€ DeepSeek-R1-0528-Qwen3-8B å› æœæ¨æ–­ç³»ç»Ÿå¯åŠ¨")
    print("=" * 70)

    # 1. èµ„æºç›‘æ§ (å¯åŠ¨å‰)
    print("\nğŸ” å¯åŠ¨å‰èµ„æºçŠ¶æ€:")
    monitor_resources()

    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸ§  åŠ è½½æ¨¡å‹...")
    model = load_model_safely(MODEL_NAME)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model.eval()  # å¯ç”¨æ¨ç†æ¨¡å¼

    # 3. éªŒè¯é…ç½®
    print("\nâœ… æ¨¡å‹é…ç½®éªŒè¯:")
    print(f"  â€¢ RoPE ç±»å‹: {getattr(model.config, 'rope_scaling', {}).get('rope_type', 'æœªè®¾ç½®')}")
    print(f"  â€¢ Flash Attention: {getattr(model.config, '_attn_implementation', 'æœªè®¾ç½®')}")
    print(f"  â€¢ é‡åŒ–ç±»å‹: {getattr(model, 'quantization_method', 'æœªé‡åŒ–')}")

    # 4. èµ„æºç›‘æ§ (åŠ è½½å)
    print("\nğŸ” æ¨¡å‹åŠ è½½åèµ„æºçŠ¶æ€:")
    monitor_resources()

    # 5. æµ‹è¯•åˆ†æ
    sample_text = (
        "å…¨çƒæ°”å€™å˜æš–å¯¼è‡´åŒ—æå†°ç›–èåŒ–åŠ é€Ÿï¼Œå¼•å‘æµ·å¹³é¢ä¸Šå‡å¨èƒæ²¿æµ·åŸå¸‚ã€‚"
        "åŒæ—¶ï¼Œå¯å†ç”Ÿèƒ½æºæŠ•èµ„å¢é•¿é™ä½äº†å¤ªé˜³èƒ½æ¿æˆæœ¬ï¼Œæ¨åŠ¨äº†ç»¿è‰²æŠ€æœ¯æ™®åŠã€‚"
    )

    print(f"\nğŸ” åˆ†ææ–‡æœ¬: {sample_text}")
    results = extract_causal_relations(model, tokenizer, sample_text)

    # 6. æ˜¾ç¤ºç»“æœ
    print("\nâœ… æå–ç»“æœ:")
    if results:
        for i, rel in enumerate(results, 1):
            print(f"\n{i}. äº‹ä»¶: {rel['event']}")
            print(f"   å®ä½“: {rel['entity']}")
            print(f"   å½±å“: {rel['effect']}")
    else:
        print("  âš ï¸  æœªæ£€æµ‹åˆ°å› æœå…³ç³»")

    # 7. ä¿å­˜ç»“æœ
    output_file = "deepseek_causal_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")

    # 8. æœ€ç»ˆèµ„æºç›‘æ§
    print("\nğŸ” æœ€ç»ˆèµ„æºçŠ¶æ€:")
    monitor_resources()

    print("\n" + "=" * 70)
    print("âœ… å› æœæ¨æ–­åˆ†æå®Œæˆ")
    print("=" * 70)


# ===== å¯åŠ¨ç¨‹åº =====
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")