import sys
import time  # <--- æ–°å¢ 1: å¯¼å…¥ time æ¨¡å—
from llama_cpp import Llama, GGML_TYPE_Q8_0
from config import default_user_prompt_template,user_input_d,news_context_text
import concurrent.futures
import json

# ================= é…ç½®åŒºåŸŸ =================
# ä½ çš„é…ç½®ä¿æŒä¸å˜
MODEL_PATH = "/home/shangong/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GGUF/snapshots/e4d4bafdfb96a411a163846265362aceb0b9c63a/Qwen3-30B-A3B-Q4_K_M.gguf"
# N_GPU_LAYERS = 35
N_GPU_LAYERS = 42
CONTEXT_SIZE = 8192


# ===========================================

def init_model():
    # ... (ä½ çš„ init_model ä»£ç ä¿æŒä¸å˜) ...
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}...")
    print(f"å°è¯•åŠ è½½ GPU å±‚æ•°: {N_GPU_LAYERS} (åˆ©ç”¨ RTX 5070 Ti 16G)")

    try:
        llm = Llama(
            model_path=MODEL_PATH,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=CONTEXT_SIZE,
            n_batch=512,
            flash_attn=True,
            type_k=GGML_TYPE_Q8_0,
            type_v=GGML_TYPE_Q8_0,
            verbose=True
        )
        return llm
    except Exception as e:
        print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        sys.exit(1)


my_llm = init_model()


def chat_stream(llm, prompt, prompt_template = None):
    """
    å¸¦é€Ÿåº¦ç»Ÿè®¡çš„æµå¼å¯¹è¯å‡½æ•°
    """
    

    messages = [
        # {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚"},
        {"role": "user", "content":  prompt_template.replace('[prompt]', prompt) if prompt_template is not None else prompt}
    ]

    print("\næ­£åœ¨æ€è€ƒ...", end="", flush=True)

    # è®°å½•å¼€å§‹å¤„ç†çš„æ—¶é—´
    start_process_time = time.time()

    # å‘èµ·æ¨ç†è¯·æ±‚
    stream = llm.create_chat_completion(
        messages=messages,
        max_tokens=2048,
        temperature=0.3,
        stream=True
    )

    print("\rAI å›å¤: ", end="", flush=True)  # \r æŠŠå‰é¢çš„"æ­£åœ¨æ€è€ƒ"è¦†ç›–æ‰

    token_count = 0
    first_token_time = None
    start_gen_time = None

    result = ''
    # å¾ªç¯è·å–æµå¼å—
    for chunk in stream:
        # print("chunk:", chunk)
        delta = chunk['choices'][0]['delta']

        if 'content' in delta:
            content = delta['content']

            # æ•è·ç¬¬ä¸€ä¸ª token çš„æ—¶é—´
            if first_token_time is None:
                first_token_time = time.time()
                start_gen_time = first_token_time  # å¼€å§‹ç”Ÿæˆçš„è®¡æ—¶èµ·ç‚¹
                # è®¡ç®—é¦–å­—å»¶è¿Ÿ (Time to First Token)
                ttft = first_token_time - start_process_time

            print(content, end="", flush=True)
            result += content
            token_count += 1

    # ç»“æŸè®¡æ—¶
    end_time = time.time()

    print("\n\n" + "=" * 30)

    # --- ç»Ÿè®¡è®¡ç®— ---
    if token_count > 0 and start_gen_time:
        # çº¯ç”Ÿæˆè€—æ—¶ (æ‰£é™¤é¦–å­—ç­‰å¾…æ—¶é—´)
        gen_duration = end_time - start_gen_time
        # é¦–å­—å»¶è¿Ÿ
        ttft = first_token_time - start_process_time

        # è®¡ç®—é€Ÿåº¦ (Tokens Per Second)
        # é˜²æ­¢é™¤ä»¥0 (è™½ç„¶ä¸å¤ªå¯èƒ½)
        speed = token_count / gen_duration if gen_duration > 0 else 0

        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"   - ç”Ÿæˆé•¿åº¦: {token_count} tokens")
        print(f"   - é¦–å­—å»¶è¿Ÿ (TTFT): {ttft:.2f} s (é¢„å¤„ç†è€—æ—¶)")
        print(f"   - ç”Ÿæˆè€—æ—¶: {gen_duration:.2f} s")
        print(f"   - å¹³å‡é€Ÿåº¦: \033[1;32m{speed:.2f} tokens/s\033[0m")  # ç»¿è‰²é«˜äº®æ˜¾ç¤ºé€Ÿåº¦
    else:
        print("æœªç”Ÿæˆæœ‰æ•ˆå†…å®¹ã€‚")
    print("=" * 30 + "\n")
    result = result.strip()
    return result


# ================= å·¥å…·å‡½æ•°ï¼šå®‰å…¨çš„ JSON è§£æ =================
def clean_and_parse_json(content):
    """å°è¯•æ¸…ç† markdown æ ‡è®°å¹¶è§£æ JSON"""
    try:
        # å»æ‰ ```json å’Œ ``` ä»¥åŠé¦–å°¾ç©ºç™½
        content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"âš ï¸ JSON è§£æè­¦å‘Š: {e}")
        return None


# ================= æ¨¡å— 1: å¸¦åæ€çš„è§„åˆ’è€… (Reflective Planner) =================

def generate_initial_plan(topic):
    """ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆç¨¿"""
    print(f"ğŸ’¡ [Draft] æ­£åœ¨èµ·è‰åˆæ­¥è®¡åˆ’: {topic} ...")
    prompt = f"""
    ç”¨æˆ·æƒ³ç ”ç©¶çš„ä¸»é¢˜æ˜¯ï¼šâ€œ{topic}â€ã€‚
    è¯·åˆ—å‡º 3-5 ä¸ªæ ¸å¿ƒå­é—®é¢˜ï¼Œå¸®åŠ©å…¨é¢ç†è§£è¿™ä¸ªä¸»é¢˜ã€‚
    åªè¿”å› JSON å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š["é—®é¢˜1", "é—®é¢˜2"]
    """
    # response = client.chat.completions.create(
    #     model=MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.7
    # )
    result = chat_stream(my_llm, prompt)
    return clean_and_parse_json(result)

def refine_plan(topic, initial_plan):
    """ç¬¬äºŒæ­¥ï¼šåæ€ä¸ä¿®è®¢ (Critique & Refine)"""
    print(f"ğŸ¤” [Refine] æ­£åœ¨åæ€å¹¶ä¼˜åŒ–è®¡åˆ’...")
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä¸»ç¼–ã€‚
    ç”¨æˆ·çš„ç ”ç©¶ä¸»é¢˜æ˜¯ï¼šâ€œ{topic}â€ã€‚
    
    è¿™æ˜¯åˆçº§ç ”ç©¶å‘˜æå‡ºçš„åˆæ­¥æœç´¢è®¡åˆ’ï¼š
    {json.dumps(initial_plan, ensure_ascii=False)}
    
    è¯·ä½ æ‰¹åˆ¤æ€§åœ°å®¡è§†è¿™ä¸ªè®¡åˆ’ï¼š
    1. æ˜¯å¦æœ‰é—æ¼çš„å…³é”®è§’åº¦ï¼Ÿ
    2. æ˜¯å¦æœ‰é‡å¤çš„å†…å®¹ï¼Ÿ
    3. é—®é¢˜çš„é¢—ç²’åº¦æ˜¯å¦åˆé€‚ï¼Ÿ(ä¸è¦å¤ªå®½æ³›ï¼Œä¹Ÿä¸è¦å¤ªç»†èŠ‚)
    
    è¯·ç»™å‡ºä¸€ä¸ª**ä¿®è®¢å**çš„ã€æ›´å®Œç¾çš„å­é—®é¢˜åˆ—è¡¨ã€‚
    ä¿æŒ JSON åˆ—è¡¨æ ¼å¼è¾“å‡ºã€‚
    """
    
    # response = client.chat.completions.create(
    #     model=MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.5
    # )
    
    # refined_plan = clean_and_parse_json(response.choices[0].message.content)
    
    result = chat_stream(my_llm, prompt)
    refined_plan= clean_and_parse_json(result)

    if refined_plan:
        print(f"âœ¨ [Refine] è®¡åˆ’å·²ä¼˜åŒ–ï¼Œä» {len(initial_plan)} ä¸ªä»»åŠ¡è°ƒæ•´ä¸º {len(refined_plan)} ä¸ªã€‚")
        return refined_plan
    else:
        print("âš ï¸ ä¼˜åŒ–è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸè®¡åˆ’ã€‚")
        return initial_plan


def get_research_plan(topic):
    """è§„åˆ’æ€»å…¥å£"""
    # 1. èµ·è‰
    draft = generate_initial_plan(topic)
    if not draft: return [topic] # å…œåº•
    
    # 2. ä¿®è®¢
    final_plan = refine_plan(topic, draft)
    return final_plan



# ================= æ¨¡å— 2: æ‰§è¡Œè€… (Worker) =================

def execute_step(sub_question):
    """å•çº¿ç¨‹å·¥ä½œå•å…ƒï¼šæœç´¢ -> æ€»ç»“"""
    try:
        print(f"ğŸ” [Search] å¼€å§‹æœ: {sub_question}")
        
        # TODOï¼šæœç´¢ (Tavily)
        # search_result = tavily.search(query=sub_question, search_depth="advanced", max_results=3)
        # context_text = "\n".join([f"- æ¥æº: {r['url']}\n  å†…å®¹: {r['content']}" for r in search_result['results']])
        
        # æ€»ç»“ (Qwen)
        print(f"ğŸ“– [Read] æ­£åœ¨é˜…è¯»å¹¶æ€»ç»“: {sub_question} ...")
        summary_prompt = f"""
        é’ˆå¯¹é—®é¢˜ï¼šâ€œ{sub_question}â€
        è¯·é˜…è¯»ä»¥ä¸‹è”ç½‘æœç´¢åˆ°çš„åŸå§‹æ•°æ®ï¼Œå†™ä¸€æ®µç»“æ„æ¸…æ™°çš„ç¬”è®°ï¼ˆçº¦ 300 å­—ï¼‰ã€‚
        ç¬”è®°å¿…é¡»åŒ…å«å…·ä½“çš„**æ•°æ®ã€æ—¥æœŸã€äººåæˆ–å…³é”®äº‹å®**ã€‚ä¸è¦å†™åºŸè¯ã€‚
        
        åŸå§‹æ•°æ®ï¼š
        {news_context_text}
        """
        
        response = chat_stream(my_llm, summary_prompt)
        #  client.chat.completions.create(
        #     model=MODEL_NAME, messages=[{"role": "user", "content": summary_prompt}], temperature=0.3
        # )
        return f"### å­è¯¾é¢˜ï¼š{sub_question}\n{response}\n"
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡å¤±è´¥: {sub_question}, é”™è¯¯: {e}")
        return f"### å­è¯¾é¢˜ï¼š{sub_question}\n(è¯¥éƒ¨åˆ†æœç´¢å¤±è´¥)\n"

# ================= æ¨¡å— 3: æ•´åˆè€… (Writer) =================

def write_final_report(topic, all_notes):
    print(f"âœï¸ [Write] æ­£åœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Š...")
    prompt = f"""
    ä½ æ˜¯ä¸€åé¡¶çº§è¡Œä¸šåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹è°ƒç ”ç¬”è®°ï¼Œå†™ä¸€ä»½å…³äºâ€œ{topic}â€çš„æ·±åº¦æŠ¥å‘Šã€‚
    
    è°ƒç ”ç¬”è®°ï¼š
    {all_notes}
    
    å†™ä½œè¦æ±‚ï¼š
    1. æ ‡é¢˜å¿…é¡»ä¸“ä¸šï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚
    2. å¼€å¤´å…ˆç»™å‡ºâ€œæ ¸å¿ƒç»“è®º (Executive Summary)â€ã€‚
    3. æ­£æ–‡é€»è¾‘åˆ†å±‚ï¼Œå¼•ç”¨ç¬”è®°ä¸­çš„æ•°æ®æ”¯æŒä½ çš„è§‚ç‚¹ã€‚
    4. è¯­æ°”å®¢è§‚ã€ä¸“ä¸šã€‚
    """
    
    response = chat_stream(my_llm, prompt)
    #  client.chat.completions.create(
    #     model=MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.5
    # )
    return response

# ================= ä¸»æµç¨‹ =================

def run_deep_research_v2(topic):
    print(f"ğŸš€ å¯åŠ¨ Deep Research V2 (å«åæ€ä¸å¹¶å‘)...")
    
    # 1. æ™ºèƒ½è§„åˆ’ (å«åæ€)
    plan = get_research_plan(topic)
    print(f"ğŸ“‹ æœ€ç»ˆæ‰§è¡Œæ¸…å•: {plan}")
    
    # 2. å¹¶å‘æ‰§è¡Œ (å¤§å¤§æå‡é€Ÿåº¦)
    all_results = []
    # ä½¿ç”¨ ThreadPoolExecutor å®ç°å¹¶å‘
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_query = {executor.submit(execute_step, q): q for q in plan}
        
        # è·å–ç»“æœ
        for future in concurrent.futures.as_completed(future_to_query):
            query = future_to_query[future]
            try:
                data = future.result()
                all_results.append(data)
            except Exception as exc:
                print(f"ä»»åŠ¡ {query} æŠ›å‡ºå¼‚å¸¸: {exc}")

    # å°†æ‰€æœ‰ç¬”è®°æ‹¼æ¥
    full_notes = "\n".join(all_results)
    
    # 3. æœ€ç»ˆå†™ä½œ
    report = write_final_report(topic, full_notes)
    
    print("\n" + "="*40)
    print(report)
    print("="*40)
    return report



if __name__ == "__main__":
    # 1. åˆå§‹åŒ–
    # my_llm = init_model()
    # chat_stream(my_llm, user_input_d)

    run_deep_research_v2("è¯„ä¼°ä¸‰ä¸€é‡å·¥è‚¡ç¥¨æœªæ¥çš„èµ°åŠ¿")

    # 2. è¿›å…¥å¾ªç¯å¯¹è¯
    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥é—®é¢˜ (è¾“å…¥ 'exit' é€€å‡º): ")
            if user_input.lower() == 'exit':
                break

            if not user_input.strip():
                continue

            chat_stream(my_llm, user_input)
        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­å¯¹è¯")