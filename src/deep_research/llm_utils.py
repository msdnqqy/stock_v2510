from pyexpat import model
import sys
import time  # <--- æ–°å¢ 1: å¯¼å…¥ time æ¨¡å—
from llama_cpp import Llama, GGML_TYPE_Q8_0
from config import default_user_prompt_template,user_input_d,news_context_text
import concurrent.futures

# ================= é…ç½®åŒºåŸŸ =================
# ä½ çš„é…ç½®ä¿æŒä¸å˜
MODEL_PATH = "/home/shangong/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GGUF/snapshots/e4d4bafdfb96a411a163846265362aceb0b9c63a/Qwen3-30B-A3B-Q4_K_M.gguf"
# N_GPU_LAYERS = 35
N_GPU_LAYERS = 42
CONTEXT_SIZE = 8192



def init_model():
    # ... (ä½ çš„ init_model ä»£ç ä¿æŒä¸å˜) ...
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}...")
    print(f"å°è¯•åŠ è½½ GPU å±‚æ•°: {N_GPU_LAYERS} (åˆ©ç”¨ RTX 5070 Ti 16G)")

    try:
        llm = Llama(
            model_path=MODEL_PATH,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=CONTEXT_SIZE,
            n_batch=512,
            flash_attn=True,
            type_k=GGML_TYPE_Q8_0,
            type_v=GGML_TYPE_Q8_0,
            verbose=True
        )
        return llm
    except Exception as e:
        print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        sys.exit(1)


my_llm = init_model()
print("æ¨¡å‹åŠ è½½æˆåŠŸ:\t",MODEL_PATH,my_llm)


def chat_stream( prompt, prompt_template = None):
    """
    å¸¦é€Ÿåº¦ç»Ÿè®¡çš„æµå¼å¯¹è¯å‡½æ•°
    """
    

    messages = [
        # {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚"},
        {"role": "user", "content":  prompt_template.replace('[prompt]', prompt) if prompt_template is not None else prompt}
    ]

    print("\næ­£åœ¨æ€è€ƒ...", end="", flush=True)

    # è®°å½•å¼€å§‹å¤„ç†çš„æ—¶é—´
    start_process_time = time.time()

    # å‘èµ·æ¨ç†è¯·æ±‚
    stream = my_llm.create_chat_completion(
        messages=messages,
        max_tokens=2048,
        temperature=0.3,
        stream=True
    )

    print("\rAI å›å¤: ", end="", flush=True)  # \r æŠŠå‰é¢çš„"æ­£åœ¨æ€è€ƒ"è¦†ç›–æ‰

    token_count = 0
    first_token_time = None
    start_gen_time = None

    result = ''
    # å¾ªç¯è·å–æµå¼å—
    for chunk in stream:
        # print("chunk:", chunk)
        delta = chunk['choices'][0]['delta']

        if 'content' in delta:
            content = delta['content']

            # æ•è·ç¬¬ä¸€ä¸ª token çš„æ—¶é—´
            if first_token_time is None:
                first_token_time = time.time()
                start_gen_time = first_token_time  # å¼€å§‹ç”Ÿæˆçš„è®¡æ—¶èµ·ç‚¹
                # è®¡ç®—é¦–å­—å»¶è¿Ÿ (Time to First Token)
                ttft = first_token_time - start_process_time

            print(content, end="", flush=True)
            result += content
            token_count += 1

    # ç»“æŸè®¡æ—¶
    end_time = time.time()

    print("\n\n" + "=" * 30)

    # --- ç»Ÿè®¡è®¡ç®— ---
    if token_count > 0 and start_gen_time:
        # çº¯ç”Ÿæˆè€—æ—¶ (æ‰£é™¤é¦–å­—ç­‰å¾…æ—¶é—´)
        gen_duration = end_time - start_gen_time
        # é¦–å­—å»¶è¿Ÿ
        ttft = first_token_time - start_process_time

        # è®¡ç®—é€Ÿåº¦ (Tokens Per Second)
        # é˜²æ­¢é™¤ä»¥0 (è™½ç„¶ä¸å¤ªå¯èƒ½)
        speed = token_count / gen_duration if gen_duration > 0 else 0

        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"   - ç”Ÿæˆé•¿åº¦: {token_count} tokens")
        print(f"   - é¦–å­—å»¶è¿Ÿ (TTFT): {ttft:.2f} s (é¢„å¤„ç†è€—æ—¶)")
        print(f"   - ç”Ÿæˆè€—æ—¶: {gen_duration:.2f} s")
        print(f"   - å¹³å‡é€Ÿåº¦: \033[1;32m{speed:.2f} tokens/s\033[0m")  # ç»¿è‰²é«˜äº®æ˜¾ç¤ºé€Ÿåº¦
    else:
        print("æœªç”Ÿæˆæœ‰æ•ˆå†…å®¹ã€‚")
    print("=" * 30 + "\n")
    result = result.strip()
    return result