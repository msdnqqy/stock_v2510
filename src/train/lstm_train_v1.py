import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from data_loader.stock_dataset_v2 import TransformedDataset

# ----------------------------
# 1. æ¨¡æ‹Ÿé‡‘èä»·æ ¼æ•°æ®ï¼ˆæˆ–æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
# ----------------------------
np.random.seed(42)
torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
import torch
from torch.utils.data import DataLoader, Subset

# 1. åˆ›å»ºå®Œæ•´æ•°æ®é›†
full_dataset = TransformedDataset(
    days=32,
    label_days=1
)

# 2. è®¡ç®—åˆ’åˆ†ç‚¹ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼ï¼‰
total_size = len(full_dataset)
train_size = int(0.8 * total_size)
val_size = total_size - train_size

print(f"Total samples: {total_size}")
print(f"Train samples: {train_size}")
print(f"Val samples: {val_size}")

# 3. åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä½¿ç”¨ç´¢å¼•åˆ‡ç‰‡ï¼‰
train_dataset = Subset(full_dataset, indices=range(0, train_size))
val_dataset = Subset(full_dataset, indices=range(train_size, total_size))

# 4. åˆ›å»º DataLoaderï¼ˆæ³¨æ„ï¼šæ—¶åºæ•°æ®é€šå¸¸ä¸ shuffle è®­ç»ƒé›†ï¼ï¼‰
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# 5. éªŒè¯åˆ’åˆ†æ˜¯å¦æ­£ç¡®
print("First train sample index:", train_dataset.indices[0])
print("Last train sample index:", train_dataset.indices[-1])
print("First val sample index:", val_dataset.indices[0])
print("Last val sample index:", val_dataset.indices[-1])


# ----------------------------
# 5. LSTM æ¨¡å‹ï¼ˆé¢„æµ‹ log returnï¼‰
# ----------------------------
class LSTMReturnPredictor(nn.Module):
    def __init__(self, input_size=6, hidden_size=2048, num_layers=4, output_size=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)  # (B, L, H)
        out = self.fc(out[:, -1, :])  # (B, 1)
        return out
        # return out.squeeze(-1)  # (B,)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMReturnPredictor().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# ----------------------------
# 6. è®­ç»ƒå¾ªç¯
# ----------------------------
epochs = 50
train_losses, val_losses = [], []

for epoch in range(epochs):
    model.train()
    total_train_loss = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        loss.backward()
        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ğŸ‘ˆ å…³é”®ï¼
        optimizer.step()
        total_train_loss += loss.item()

    # éªŒè¯
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch)
            total_val_loss += loss.item()

    train_losses.append(total_train_loss / len(train_loader))
    val_losses.append(total_val_loss / len(val_loader))

    if True or (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}")

# ----------------------------
# 7. é¢„æµ‹å¹¶è¿˜åŸä¸ºç»å¯¹ä»·æ ¼
# ----------------------------
# å–éªŒè¯é›†æœ€åä¸€ä¸ªçª—å£ä½œä¸ºèµ·ç‚¹
# last_window = X_val[-1:]  # shape: (1, seq_len, 1)
# last_price = price[split + seq_len + len(y_train)]  # å¯¹åº”é¢„æµ‹èµ·ç‚¹çš„çœŸå®ä»·æ ¼
#
# model.eval()
# with torch.no_grad():
#     pred_log_return = model(torch.tensor(last_window, dtype=torch.float32).to(device)).item()
#
# # è¿˜åŸé¢„æµ‹ä»·æ ¼
# pred_price = last_price * np.exp(pred_log_return)
# true_price = price[split + seq_len + len(y_train) + 1]  # çœŸå®ä¸‹ä¸€æœŸä»·æ ¼
#
# print(f"\né¢„æµ‹ log return: {pred_log_return:.6f}")
# print(f"åŸºäºä»·æ ¼ {last_price:.2f}ï¼Œé¢„æµ‹ä¸‹ä¸€æœŸä»·æ ¼: {pred_price:.2f}")
# print(f"çœŸå®ä»·æ ¼: {true_price:.2f}")
# print(f"é¢„æµ‹è¯¯å·®: {abs(pred_price - true_price):.2f}")
#
# # ----------------------------
# # 8. ï¼ˆå¯é€‰ï¼‰å¯è§†åŒ–è®­ç»ƒæŸå¤±
# # ----------------------------
# plt.figure(figsize=(10, 4))
# plt.plot(train_losses, label='Train Loss')
# plt.plot(val_losses, label='Val Loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epoch')
# plt.ylabel('MSE Loss')
# plt.legend()
# plt.grid(True)
# plt.show()