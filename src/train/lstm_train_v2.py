import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/mnt/d/projects/stock_v2510', '/mnt/d/projects/stock_v2510/src'])

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from data_loader.stock_dataset_v2 import TransformedDataset

# ----------------------------
# 1. æ¨¡æ‹Ÿé‡‘èä»·æ ¼æ•°æ®ï¼ˆæˆ–æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
# ----------------------------
np.random.seed(42)
torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
import torch
from torch.utils.data import DataLoader, Subset


def get_datasets(paths=['./data/stock_pre/SH#600031.csv']):
    result=[]
    for path in paths:
        # 1. åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = TransformedDataset(
            file_paths = path,
            days=32,
            label_days=1
        )

        # 2. è®¡ç®—åˆ’åˆ†ç‚¹ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼ï¼‰
        total_size = len(full_dataset)
        train_size = int(0.8 * total_size)
        val_size = total_size - train_size

        print(f"Total samples: {total_size}")
        print(f"Train samples: {train_size}")
        print(f"Val samples: {val_size}")

        # 3. åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä½¿ç”¨ç´¢å¼•åˆ‡ç‰‡ï¼‰
        train_dataset = Subset(full_dataset, indices=range(0, train_size))
        val_dataset = Subset(full_dataset, indices=range(train_size, total_size))

        # 4. åˆ›å»º DataLoaderï¼ˆæ³¨æ„ï¼šæ—¶åºæ•°æ®é€šå¸¸ä¸ shuffle è®­ç»ƒé›†ï¼ï¼‰
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

        # 5. éªŒè¯åˆ’åˆ†æ˜¯å¦æ­£ç¡®
        print("First train sample index:", train_dataset.indices[0])
        print("Last train sample index:", train_dataset.indices[-1])
        print("First val sample index:", val_dataset.indices[0])
        print("Last val sample index:", val_dataset.indices[-1])

        result.append((train_loader, val_loader))

    return result


data_loaders = get_datasets(paths = ['./data/stock_pre/SH#600031.csv'
    ,'./data/stock_pre/SH#600036.csv'

    # runï¼ˆ1ï¼‰
    ,'./data/stock_pre/SH#600048.csv'
    ,'./data/stock_pre/SH#600050.csv'
    ,'./data/stock_pre/SH#600111.csv'

    # run(2)
    ,'./data/stock_pre/SH#600276.csv'
    ,'./data/stock_pre/SH#600309.csv'
    ,'./data/stock_pre/SH#600406.csv'
    ,'./data/stock_pre/SH#600415.csv'
    ,'./data/stock_pre/SH#600426.csv'
    ,'./data/stock_pre/SH#600436.csv'
    ,'./data/stock_pre/SH#600519.csv'

    # ,'./data/stock_pre/SH#600585.csv'
    # ,'./data/stock_pre/SH#600660.csv'
    # ,'./data/stock_pre/SH#600690.csv'
    # ,'./data/stock_pre/SH#600809.csv'
    # ,'./data/stock_pre/SH#600887.csv'
    # ,'./data/stock_pre/SH#600893.csv'
    # ,'./data/stock_pre/SH#600900.csv'
    # ,'./data/stock_pre/SH#600919.csv'
    # ,'./data/stock_pre/SH#600941.csv'
    # ,'./data/stock_pre/SH#601012.csv'
    # ,'./data/stock_pre/SH#601088.csv'
    # ,'./data/stock_pre/SH#601138.csv'
    # ,'./data/stock_pre/SH#601166.csv'
                                     ])
# ----------------------------
# 5. LSTM æ¨¡å‹ï¼ˆé¢„æµ‹ log returnï¼‰
# ----------------------------
class LSTMReturnPredictor(nn.Module):
    def __init__(self, input_size=6, hidden_size=1024, num_layers=3, output_size=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)

        self.net = nn.Sequential(
            nn.Linear(hidden_size, 2048),
            nn.ReLU(),
            # nn.Linear(2048, 2048),
            # nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2048, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, output_size)  # è¾“å‡º logits
        )


    def forward(self, x):
        out, _ = self.lstm(x)  # (B, L, H)
        out = self.net(out[:, -1, :])  # (B, 1)

        return out
        # return out.squeeze(-1)  # (B,)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMReturnPredictor().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# ----------------------------
# 6. è®­ç»ƒå¾ªç¯
# ----------------------------
epochs = 50
train_losses, val_losses = [], []

for epoch in range(epochs):
    model.train()
    total_train_loss = 0

    for data_loader in data_loaders:
        for X_batch, y_batch in data_loader[0]:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ğŸ‘ˆ å…³é”®ï¼
            optimizer.step()
            total_train_loss += loss.item()

    # éªŒè¯
    model.eval()
    total_val_loss = 0
    for data_loader in data_loaders:
        with torch.no_grad():
            for X_batch, y_batch in data_loader[1]:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                y_pred = model(X_batch)
                loss = criterion(y_pred, y_batch)
                total_val_loss += loss.item()
        break

    # print(total_train_loss,len(train_loader),total_val_loss,len(val_loader))
    train_losses.append(total_train_loss / np.array([len(data_loader[0]) for data_loader in data_loaders]).sum())
    val_losses.append(total_val_loss /  np.array([len(data_loader[1]) for data_loader in data_loaders]).sum())

    len_val = 0
    for data_loader in data_loaders:
        len_val+=len(data_loader[1])
        break
        
    val_losses.append(total_val_loss /  len_val)

    if True or (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}")

# ----------------------------
# 7. é¢„æµ‹å¹¶è¿˜åŸä¸ºç»å¯¹ä»·æ ¼
# ----------------------------
# å–éªŒè¯é›†æœ€åä¸€ä¸ªçª—å£ä½œä¸ºèµ·ç‚¹
# last_window = X_val[-1:]  # shape: (1, seq_len, 1)
# last_price = price[split + seq_len + len(y_train)]  # å¯¹åº”é¢„æµ‹èµ·ç‚¹çš„çœŸå®ä»·æ ¼
#
# model.eval()
# with torch.no_grad():
#     pred_log_return = model(torch.tensor(last_window, dtype=torch.float32).to(device)).item()
#
# # è¿˜åŸé¢„æµ‹ä»·æ ¼
# pred_price = last_price * np.exp(pred_log_return)
# true_price = price[split + seq_len + len(y_train) + 1]  # çœŸå®ä¸‹ä¸€æœŸä»·æ ¼
#
# print(f"\né¢„æµ‹ log return: {pred_log_return:.6f}")
# print(f"åŸºäºä»·æ ¼ {last_price:.2f}ï¼Œé¢„æµ‹ä¸‹ä¸€æœŸä»·æ ¼: {pred_price:.2f}")
# print(f"çœŸå®ä»·æ ¼: {true_price:.2f}")
# print(f"é¢„æµ‹è¯¯å·®: {abs(pred_price - true_price):.2f}")
#
# # ----------------------------
# # 8. ï¼ˆå¯é€‰ï¼‰å¯è§†åŒ–è®­ç»ƒæŸå¤±
# # ----------------------------
# plt.figure(figsize=(10, 4))
# plt.plot(train_losses, label='Train Loss')
# plt.plot(val_losses, label='Val Loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epoch')
# plt.ylabel('MSE Loss')
# plt.legend()
# plt.grid(True)
# plt.show()